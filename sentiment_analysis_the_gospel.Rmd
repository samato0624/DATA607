---
title: "DATA607_SA_WEEK10_HW"
author: "Sean Amato"
date: "2023-11-11"
output: html_document
---

## Overview
For my assignment I recreated the sentiment analysis, performed in Section 2.2 "Sentiment analysis with inner join", 2.3 "Comparing the three sentiment dictionaries" and 2.5 "Wordclouds", from our textbook "Welcome to Text Mining with R". The text I used was the King James version of the bible. I wanted to get an intuitive understanding of whether the text is really "hell fire and brimstone" or not.

Citation: Julia Silge and David Robinson, 2022-11-02, Welcome to Text Mining with R, and Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License.

NOTE: use AFINN (scored), bing (binary), or nrc (emotions) as the lexicon of words.

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(tidytext)
library(textdata)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(DT)
library(wordcloud)
library(reshape2)
library(vader)
```

Here I'm just putting the text into a dataframe and parsing the words into separate rows.
```{r}
scripture = sacred::king_james_version
scripture <- as.data.frame(scripture)

tidy_books <- scripture %>%
  group_by(book) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

Here I'm looking at the counts of words with a joyful sentiment.

Here "god" is referenced the most which is not really surprise.
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
    inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Here I'm looking at the Gospel with each book visualized as segmented sentiments using the bing lexicon.

It's interesting to note that each book seems to start positively, but appears mostly negative in each book during the second half, which could be mostly attributed to the story of the crucifixion. 
```{r}
the_gospel <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(linenumber = row_number()) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  filter(book == "mat" | book == "mar" | book == "luk" | book == "joh")



ggplot(the_gospel, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

Here I'm comparing all the lexicons for just the book of Mark, as it appeared to have the largest negative sentiment.

Based on the bar charts below we can see that AFFIN and Bing are relatively similar and NRC reflects the relative difference, i.e. the book's timeline is perceived similarly across the 3 different lexicons.
```{r}
gospel_mark <- tidy_books %>% 
  filter(book == "mar")

afinn <- gospel_mark %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  gospel_mark %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  gospel_mark %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

Here I used a wordcloud to better visualize word contents and it seems the scriptures have more positive words than negative, in terms of the whole bible.
```{r}
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Here I tried using the VADER lexicon, which is used primarily for sentiment analysis on social media. Due to the way the get_vader method works I can really only do 1000 words at a time. When I attempted to put the whole book of Matthew my program stopped working. The results are jarring but in essense 7.6% of the words were deemed positive, 81.7% were neutral, and 5.3% were negative.
```{r}
book_of_matthew <- tidy_books %>%
  filter(book == "mat")

subset_of_mat <- book_of_matthew[1:1000, ]

concatenated_mat <- paste(subset_of_mat$word, collapse = " ")
results <- get_vader(concatenated_mat)
print(results)
```